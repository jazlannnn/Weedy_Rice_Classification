{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA 11.8\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\yolov5_env\\WeedyRice_Classification\\yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI 15\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['setuptools>=70.0.0'] not found, attempting AutoUpdate...\n",
      "Collecting setuptools>=70.0.0\n",
      "\n",
      "  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "\n",
      "Downloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.5/1.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.0/1.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 2.1 MB/s eta 0:00:00\n",
      "\n",
      "Installing collected packages: setuptools\n",
      "\n",
      "  Attempting uninstall: setuptools\n",
      "\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "\n",
      "Successfully installed setuptools-75.6.0\n",
      "\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 14.2s, installed 1 package: ['setuptools>=70.0.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mclassify\\train: \u001b[0mmodel=yolov5s-cls.pt, data=../dataset, epochs=5, batch_size=64, imgsz=224, nosave=False, cache=ram, device=, workers=8, project=runs\\train-cls, name=exp, exist_ok=False, pretrained=True, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\msi 15\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\msi 15\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\msi 15\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.10.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train-cls', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0m not found, install with `pip install albumentations` (recommended)\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to yolov5s-cls.pt...\n",
      "\n",
      "  0%|          | 0.00/10.5M [00:00<?, ?B/s]\n",
      "  1%|          | 128k/10.5M [00:00<00:14, 737kB/s]\n",
      "  4%|▎         | 384k/10.5M [00:00<00:10, 1.01MB/s]\n",
      "  6%|▌         | 640k/10.5M [00:00<00:07, 1.34MB/s]\n",
      "  8%|▊         | 896k/10.5M [00:00<00:07, 1.35MB/s]\n",
      " 11%|█         | 1.12M/10.5M [00:00<00:07, 1.38MB/s]\n",
      " 13%|█▎        | 1.38M/10.5M [00:01<00:09, 1.03MB/s]\n",
      " 15%|█▌        | 1.62M/10.5M [00:01<00:07, 1.19MB/s]\n",
      " 18%|█▊        | 1.88M/10.5M [00:01<00:06, 1.43MB/s]\n",
      " 20%|██        | 2.12M/10.5M [00:01<00:06, 1.41MB/s]\n",
      " 23%|██▎       | 2.38M/10.5M [00:02<00:08, 1.06MB/s]\n",
      " 25%|██▍       | 2.62M/10.5M [00:02<00:07, 1.13MB/s]\n",
      " 27%|██▋       | 2.88M/10.5M [00:02<00:06, 1.17MB/s]\n",
      " 29%|██▊       | 3.00M/10.5M [00:02<00:07, 1.03MB/s]\n",
      " 31%|███       | 3.25M/10.5M [00:02<00:06, 1.25MB/s]\n",
      " 33%|███▎      | 3.50M/10.5M [00:02<00:05, 1.41MB/s]\n",
      " 36%|███▌      | 3.75M/10.5M [00:03<00:05, 1.36MB/s]\n",
      " 38%|███▊      | 4.00M/10.5M [00:03<00:05, 1.35MB/s]\n",
      " 40%|████      | 4.25M/10.5M [00:03<00:05, 1.19MB/s]\n",
      " 43%|████▎     | 4.50M/10.5M [00:03<00:05, 1.23MB/s]\n",
      " 45%|████▌     | 4.75M/10.5M [00:04<00:04, 1.25MB/s]\n",
      " 48%|████▊     | 5.00M/10.5M [00:04<00:04, 1.40MB/s]\n",
      " 50%|████▉     | 5.25M/10.5M [00:04<00:03, 1.43MB/s]\n",
      " 52%|█████▏    | 5.50M/10.5M [00:04<00:03, 1.34MB/s]\n",
      " 55%|█████▍    | 5.75M/10.5M [00:04<00:03, 1.35MB/s]\n",
      " 57%|█████▋    | 6.00M/10.5M [00:05<00:03, 1.31MB/s]\n",
      " 59%|█████▉    | 6.25M/10.5M [00:05<00:03, 1.36MB/s]\n",
      " 62%|██████▏   | 6.50M/10.5M [00:05<00:03, 1.36MB/s]\n",
      " 64%|██████▍   | 6.75M/10.5M [00:05<00:02, 1.52MB/s]\n",
      " 67%|██████▋   | 7.00M/10.5M [00:05<00:02, 1.45MB/s]\n",
      " 69%|██████▉   | 7.25M/10.5M [00:06<00:03, 1.02MB/s]\n",
      " 70%|███████   | 7.38M/10.5M [00:06<00:03, 1.02MB/s]\n",
      " 72%|███████▏  | 7.62M/10.5M [00:06<00:02, 1.07MB/s]\n",
      " 74%|███████▎  | 7.75M/10.5M [00:06<00:03, 849kB/s] \n",
      " 75%|███████▍  | 7.88M/10.5M [00:06<00:03, 821kB/s]\n",
      " 77%|███████▋  | 8.12M/10.5M [00:07<00:02, 940kB/s]\n",
      " 81%|████████  | 8.50M/10.5M [00:07<00:01, 1.19MB/s]\n",
      " 84%|████████▍ | 8.88M/10.5M [00:07<00:01, 1.33MB/s]\n",
      " 87%|████████▋ | 9.12M/10.5M [00:07<00:01, 1.46MB/s]\n",
      " 89%|████████▉ | 9.38M/10.5M [00:07<00:00, 1.55MB/s]\n",
      " 91%|█████████▏| 9.62M/10.5M [00:08<00:00, 1.52MB/s]\n",
      " 94%|█████████▍| 9.88M/10.5M [00:08<00:00, 1.64MB/s]\n",
      " 96%|█████████▌| 10.1M/10.5M [00:08<00:00, 1.71MB/s]\n",
      " 99%|█████████▊| 10.4M/10.5M [00:08<00:00, 1.75MB/s]\n",
      "100%|██████████| 10.5M/10.5M [00:08<00:00, 1.30MB/s]\n",
      "\n",
      "Model summary: 149 layers, 4175042 parameters, 4175042 gradients, 10.5 GFLOPs\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n",
      "c:\\yolov5_env\\WeedyRice_Classification\\yolov5\\classify\\train.py:201: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=cuda)\n",
      "Image sizes 224 train, 224 test\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\train-cls\\exp\u001b[0m\n",
      "Starting yolov5s-cls.pt training on ..\\dataset dataset with 2 classes for 5 epochs...\n",
      "\n",
      "     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]c:\\yolov5_env\\WeedyRice_Classification\\yolov5\\classify\\train.py:222: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=cuda):  # stability issues when enabled\n",
      "\n",
      "       1/5     1.36G       0.812                                    :  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]\n",
      "       1/5     1.36G       0.638                                    :  50%|█████     | 2/4 [00:03<00:02,  1.35s/it]\n",
      "       1/5     1.36G       0.633                                    :  75%|███████▌  | 3/4 [00:03<00:00,  1.26it/s]\n",
      "       1/5     1.36G       0.554                          validating:   0%|          | 0/1 [00:00<?, ?it/s]C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\classify\\val.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=device.type != \"cpu\"):\n",
      "\n",
      "       1/5     1.36G       0.554                          validating: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "                                                                                                                   \n",
      "\n",
      "       1/5     1.36G       0.554       0.287        0.95           1: 100%|██████████| 4/4 [00:03<00:00,  1.71it/s]\n",
      "       1/5     1.36G       0.554       0.287        0.95           1: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "       2/5     1.36G       0.281                                    :  25%|██▌       | 1/4 [00:00<00:00,  8.65it/s]\n",
      "       2/5     1.36G       0.308                                    :  50%|█████     | 2/4 [00:00<00:00,  8.83it/s]\n",
      "       2/5     1.36G       0.288                          validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                                                           \n",
      "\n",
      "       2/5     1.36G       0.288       0.395       0.917           1: 100%|██████████| 4/4 [00:00<00:00,  8.44it/s]\n",
      "       2/5     1.36G       0.288       0.395       0.917           1: 100%|██████████| 4/4 [00:00<00:00,  8.49it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "       3/5     1.36G       0.287                                    :  25%|██▌       | 1/4 [00:00<00:00,  8.31it/s]\n",
      "       3/5     1.36G         0.3                                    :  50%|█████     | 2/4 [00:00<00:00,  7.88it/s]\n",
      "       3/5     1.36G       0.303                                    :  75%|███████▌  | 3/4 [00:00<00:00,  8.66it/s]\n",
      "       3/5     1.36G       0.283                          validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                                                           \n",
      "\n",
      "       3/5     1.36G       0.283       0.381        0.95           1: 100%|██████████| 4/4 [00:00<00:00,  7.26it/s]\n",
      "       3/5     1.36G       0.283       0.381        0.95           1: 100%|██████████| 4/4 [00:00<00:00,  7.61it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "       4/5     1.36G       0.264                                    :  25%|██▌       | 1/4 [00:00<00:00,  7.79it/s]\n",
      "       4/5     1.36G       0.257                                    :  50%|█████     | 2/4 [00:00<00:00,  8.04it/s]\n",
      "       4/5     1.36G       0.254                                    :  75%|███████▌  | 3/4 [00:00<00:00,  8.66it/s]\n",
      "       4/5     1.36G       0.252                          validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                                                           \n",
      "\n",
      "       4/5     1.36G       0.252       0.367       0.933           1: 100%|██████████| 4/4 [00:00<00:00,  7.33it/s]\n",
      "       4/5     1.36G       0.252       0.367       0.933           1: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "       5/5     1.36G       0.226                                    :  25%|██▌       | 1/4 [00:00<00:00,  3.11it/s]\n",
      "       5/5     1.36G       0.225                                    :  50%|█████     | 2/4 [00:00<00:00,  4.88it/s]\n",
      "       5/5     1.36G       0.224                          validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "                                                                                                           \n",
      "\n",
      "       5/5     1.36G       0.224       0.357       0.917           1: 100%|██████████| 4/4 [00:00<00:00,  6.77it/s]\n",
      "       5/5     1.36G       0.224       0.357       0.917           1: 100%|██████████| 4/4 [00:00<00:00,  5.95it/s]\n",
      "\n",
      "Training complete (0.003 hours)\n",
      "Results saved to \u001b[1mruns\\train-cls\\exp\u001b[0m\n",
      "Predict:         python classify/predict.py --weights runs\\train-cls\\exp\\weights\\best.pt --source im.jpg\n",
      "Validate:        python classify/val.py --weights runs\\train-cls\\exp\\weights\\best.pt --data ..\\dataset\n",
      "Export:          python export.py --weights runs\\train-cls\\exp\\weights\\best.pt --include onnx\n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs\\train-cls\\exp\\weights\\best.pt')\n",
      "Visualize:       https://netron.app\n",
      "\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n",
      "libpng warning: iCCP: profile 'icc': 0h: PCS illuminant is not D50\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s Classification on dataset directory for 5 epochs\n",
    "\n",
    "!python classify/train.py --model yolov5s-cls.pt --data ../dataset --epochs 5 --img 224 --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.89788,  1.14860]], device='cuda:0')\n",
      "tensor([[-1.54935,  0.92175]], device='cuda:0')\n",
      "tensor([[-3.17570,  2.14053]], device='cuda:0')\n",
      "tensor([[ 2.08096, -2.54075]], device='cuda:0')\n",
      "tensor([[ 1.59656, -1.04070]], device='cuda:0')\n",
      "tensor([[ 1.52284, -1.09146]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mclassify\\predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=../test, data=data\\coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs\\predict-cls, name=exp_images, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v7.0-389-ge62a31b6 Python-3.10.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 117 layers, 4169250 parameters, 0 gradients, 10.4 GFLOPs\n",
      "image 1/6 C:\\yolov5_env\\WeedyRice_Classification\\test\\1.jpg: 224x224 Weedy Rice 0.95, Cultivated Rice 0.05, 100.3ms\n",
      "image 2/6 C:\\yolov5_env\\WeedyRice_Classification\\test\\2.png: 224x224 Weedy Rice 0.92, Cultivated Rice 0.08, 4.1ms\n",
      "image 3/6 C:\\yolov5_env\\WeedyRice_Classification\\test\\3.png: 224x224 Weedy Rice 1.00, Cultivated Rice 0.00, 6.1ms\n",
      "image 4/6 C:\\yolov5_env\\WeedyRice_Classification\\test\\4.jpg: 224x224 Cultivated Rice 0.99, Weedy Rice 0.01, 4.0ms\n",
      "image 5/6 C:\\yolov5_env\\WeedyRice_Classification\\test\\5.jpg: 224x224 Cultivated Rice 0.93, Weedy Rice 0.07, 4.0ms\n",
      "image 6/6 C:\\yolov5_env\\WeedyRice_Classification\\test\\6.jpg: 224x224 Cultivated Rice 0.93, Weedy Rice 0.07, 4.2ms\n",
      "Speed: 2.0ms pre-process, 20.4ms inference, 2.0ms NMS per image at shape (1, 3, 224, 224)\n",
      "Results saved to \u001b[1mruns\\predict-cls\\exp_images\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --img 224 --source ../test  --name exp_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Pytorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI 15\\AppData\\Local\\Temp\\ipykernel_16856\\2129334730.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_data = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "--- Model Metadata ---\n",
      "Keys in model file: dict_keys(['epoch', 'best_fitness', 'model', 'ema', 'updates', 'optimizer', 'opt', 'git', 'date'])\n",
      "\n",
      "--- Model Summary ---\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]           3,456\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "              Conv-4         [-1, 32, 112, 112]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          18,432\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              SiLU-7           [-1, 64, 56, 56]               0\n",
      "              Conv-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 32, 56, 56]           2,048\n",
      "      BatchNorm2d-10           [-1, 32, 56, 56]              64\n",
      "             SiLU-11           [-1, 32, 56, 56]               0\n",
      "             Conv-12           [-1, 32, 56, 56]               0\n",
      "           Conv2d-13           [-1, 32, 56, 56]           1,024\n",
      "      BatchNorm2d-14           [-1, 32, 56, 56]              64\n",
      "             SiLU-15           [-1, 32, 56, 56]               0\n",
      "             Conv-16           [-1, 32, 56, 56]               0\n",
      "           Conv2d-17           [-1, 32, 56, 56]           9,216\n",
      "      BatchNorm2d-18           [-1, 32, 56, 56]              64\n",
      "             SiLU-19           [-1, 32, 56, 56]               0\n",
      "             Conv-20           [-1, 32, 56, 56]               0\n",
      "       Bottleneck-21           [-1, 32, 56, 56]               0\n",
      "           Conv2d-22           [-1, 32, 56, 56]           2,048\n",
      "      BatchNorm2d-23           [-1, 32, 56, 56]              64\n",
      "             SiLU-24           [-1, 32, 56, 56]               0\n",
      "             Conv-25           [-1, 32, 56, 56]               0\n",
      "           Conv2d-26           [-1, 64, 56, 56]           4,096\n",
      "      BatchNorm2d-27           [-1, 64, 56, 56]             128\n",
      "             SiLU-28           [-1, 64, 56, 56]               0\n",
      "             Conv-29           [-1, 64, 56, 56]               0\n",
      "               C3-30           [-1, 64, 56, 56]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             SiLU-33          [-1, 128, 28, 28]               0\n",
      "             Conv-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35           [-1, 64, 28, 28]           8,192\n",
      "      BatchNorm2d-36           [-1, 64, 28, 28]             128\n",
      "             SiLU-37           [-1, 64, 28, 28]               0\n",
      "             Conv-38           [-1, 64, 28, 28]               0\n",
      "           Conv2d-39           [-1, 64, 28, 28]           4,096\n",
      "      BatchNorm2d-40           [-1, 64, 28, 28]             128\n",
      "             SiLU-41           [-1, 64, 28, 28]               0\n",
      "             Conv-42           [-1, 64, 28, 28]               0\n",
      "           Conv2d-43           [-1, 64, 28, 28]          36,864\n",
      "      BatchNorm2d-44           [-1, 64, 28, 28]             128\n",
      "             SiLU-45           [-1, 64, 28, 28]               0\n",
      "             Conv-46           [-1, 64, 28, 28]               0\n",
      "       Bottleneck-47           [-1, 64, 28, 28]               0\n",
      "           Conv2d-48           [-1, 64, 28, 28]           4,096\n",
      "      BatchNorm2d-49           [-1, 64, 28, 28]             128\n",
      "             SiLU-50           [-1, 64, 28, 28]               0\n",
      "             Conv-51           [-1, 64, 28, 28]               0\n",
      "           Conv2d-52           [-1, 64, 28, 28]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 28, 28]             128\n",
      "             SiLU-54           [-1, 64, 28, 28]               0\n",
      "             Conv-55           [-1, 64, 28, 28]               0\n",
      "       Bottleneck-56           [-1, 64, 28, 28]               0\n",
      "           Conv2d-57           [-1, 64, 28, 28]           8,192\n",
      "      BatchNorm2d-58           [-1, 64, 28, 28]             128\n",
      "             SiLU-59           [-1, 64, 28, 28]               0\n",
      "             Conv-60           [-1, 64, 28, 28]               0\n",
      "           Conv2d-61          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
      "             SiLU-63          [-1, 128, 28, 28]               0\n",
      "             Conv-64          [-1, 128, 28, 28]               0\n",
      "               C3-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-67          [-1, 256, 14, 14]             512\n",
      "             SiLU-68          [-1, 256, 14, 14]               0\n",
      "             Conv-69          [-1, 256, 14, 14]               0\n",
      "           Conv2d-70          [-1, 128, 14, 14]          32,768\n",
      "      BatchNorm2d-71          [-1, 128, 14, 14]             256\n",
      "             SiLU-72          [-1, 128, 14, 14]               0\n",
      "             Conv-73          [-1, 128, 14, 14]               0\n",
      "           Conv2d-74          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-75          [-1, 128, 14, 14]             256\n",
      "             SiLU-76          [-1, 128, 14, 14]               0\n",
      "             Conv-77          [-1, 128, 14, 14]               0\n",
      "           Conv2d-78          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-79          [-1, 128, 14, 14]             256\n",
      "             SiLU-80          [-1, 128, 14, 14]               0\n",
      "             Conv-81          [-1, 128, 14, 14]               0\n",
      "       Bottleneck-82          [-1, 128, 14, 14]               0\n",
      "           Conv2d-83          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-84          [-1, 128, 14, 14]             256\n",
      "             SiLU-85          [-1, 128, 14, 14]               0\n",
      "             Conv-86          [-1, 128, 14, 14]               0\n",
      "           Conv2d-87          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-88          [-1, 128, 14, 14]             256\n",
      "             SiLU-89          [-1, 128, 14, 14]               0\n",
      "             Conv-90          [-1, 128, 14, 14]               0\n",
      "       Bottleneck-91          [-1, 128, 14, 14]               0\n",
      "           Conv2d-92          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-93          [-1, 128, 14, 14]             256\n",
      "             SiLU-94          [-1, 128, 14, 14]               0\n",
      "             Conv-95          [-1, 128, 14, 14]               0\n",
      "           Conv2d-96          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-97          [-1, 128, 14, 14]             256\n",
      "             SiLU-98          [-1, 128, 14, 14]               0\n",
      "             Conv-99          [-1, 128, 14, 14]               0\n",
      "      Bottleneck-100          [-1, 128, 14, 14]               0\n",
      "          Conv2d-101          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-102          [-1, 128, 14, 14]             256\n",
      "            SiLU-103          [-1, 128, 14, 14]               0\n",
      "            Conv-104          [-1, 128, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]          65,536\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "            SiLU-107          [-1, 256, 14, 14]               0\n",
      "            Conv-108          [-1, 256, 14, 14]               0\n",
      "              C3-109          [-1, 256, 14, 14]               0\n",
      "          Conv2d-110            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
      "            SiLU-112            [-1, 512, 7, 7]               0\n",
      "            Conv-113            [-1, 512, 7, 7]               0\n",
      "          Conv2d-114            [-1, 256, 7, 7]         131,072\n",
      "     BatchNorm2d-115            [-1, 256, 7, 7]             512\n",
      "            SiLU-116            [-1, 256, 7, 7]               0\n",
      "            Conv-117            [-1, 256, 7, 7]               0\n",
      "          Conv2d-118            [-1, 256, 7, 7]          65,536\n",
      "     BatchNorm2d-119            [-1, 256, 7, 7]             512\n",
      "            SiLU-120            [-1, 256, 7, 7]               0\n",
      "            Conv-121            [-1, 256, 7, 7]               0\n",
      "          Conv2d-122            [-1, 256, 7, 7]         589,824\n",
      "     BatchNorm2d-123            [-1, 256, 7, 7]             512\n",
      "            SiLU-124            [-1, 256, 7, 7]               0\n",
      "            Conv-125            [-1, 256, 7, 7]               0\n",
      "      Bottleneck-126            [-1, 256, 7, 7]               0\n",
      "          Conv2d-127            [-1, 256, 7, 7]         131,072\n",
      "     BatchNorm2d-128            [-1, 256, 7, 7]             512\n",
      "            SiLU-129            [-1, 256, 7, 7]               0\n",
      "            Conv-130            [-1, 256, 7, 7]               0\n",
      "          Conv2d-131            [-1, 512, 7, 7]         262,144\n",
      "     BatchNorm2d-132            [-1, 512, 7, 7]           1,024\n",
      "            SiLU-133            [-1, 512, 7, 7]               0\n",
      "            Conv-134            [-1, 512, 7, 7]               0\n",
      "              C3-135            [-1, 512, 7, 7]               0\n",
      "          Conv2d-136           [-1, 1280, 7, 7]         655,360\n",
      "     BatchNorm2d-137           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-138           [-1, 1280, 7, 7]               0\n",
      "            Conv-139           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-140           [-1, 1280, 1, 1]               0\n",
      "         Dropout-141                 [-1, 1280]               0\n",
      "          Linear-142                    [-1, 2]           2,562\n",
      "        Classify-143                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 4,175,042\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,175,042\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 71.32\n",
      "Params size (MB): 15.93\n",
      "Estimated Total Size (MB): 87.82\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--- Loading PyTorch Preprocessed Input and Logits ---\n",
      "Preprocessed input and logits loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\yolov5_env\\WeedyRice_Classification\\yolov5\\models\\yolo.py:167: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "c:\\yolov5_env\\WeedyRice_Classification\\yolov5\\models\\yolo.py:171: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if visualize:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to ONNX successfully at C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\\model.onnx!\n",
      "ONNX Model tested successfully!\n",
      "\n",
      "--- Comparing PyTorch and ONNX Outputs ---\n",
      "Logit Differences: [[  0.0014409   0.0011091]]\n",
      "Probability Differences: [[ 0.00016224  0.00016229]]\n",
      "PyTorch Predicted Class: [0]\n",
      "ONNX Predicted Class: [0]\n",
      "Do predicted classes match? True\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "#before and during convert to onnx\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the model file path\n",
    "model_path = r'C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\\best.pt'\n",
    "\n",
    "# Define the paths for the input and logits files\n",
    "pytorch_input_path = r'C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\pytorch_input.pt'\n",
    "pytorch_logits_path = r'C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\pytorch_logits.pt'\n",
    "\n",
    "\n",
    "# Define the directory and file path for the ONNX model\n",
    "onnx_model_dir = r'C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights'\n",
    "onnx_model_path = os.path.join(onnx_model_dir, \"model.onnx\")\n",
    "\n",
    "# Define preprocessing identical to `predict.py`\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Match input size\n",
    "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Example mean/std\n",
    "])\n",
    "\n",
    "# Step 1: Load the model\n",
    "try:\n",
    "    model_data = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model = model_data['model']\n",
    "    model = model.float().to('cpu')  # Ensure model is in full precision and on CPU\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Inspect Metadata\n",
    "print(\"\\n--- Model Metadata ---\")\n",
    "if isinstance(model_data, dict):\n",
    "    print(f\"Keys in model file: {model_data.keys()}\")\n",
    "\n",
    "# Step 3: Model Summary\n",
    "print(\"\\n--- Model Summary ---\")\n",
    "try:\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(torch.device('cpu'))  # Ensure input is on CPU\n",
    "    model.to('cpu')  # Ensure model is also on CPU\n",
    "    summary(model, input_size=(3, 224, 224), device=\"cpu\")  # Specify device explicitly\n",
    "except Exception as e:\n",
    "    print(f\"Unable to summarize model: {e}\")\n",
    "\n",
    "\n",
    "# Step 4: Load Preprocessed Input and Logits\n",
    "print(\"\\n--- Loading PyTorch Preprocessed Input and Logits ---\")\n",
    "try:\n",
    "    pytorch_input = torch.load(pytorch_input_path).to('cpu')  # Load and move to CPU\n",
    "    pytorch_logits = torch.load(pytorch_logits_path).to('cpu')  # Load and move to CPU\n",
    "    print(\"Preprocessed input and logits loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading preprocessed input or logits: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 5: Export PyTorch Model to ONNX\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        pytorch_input,  # Use preprocessed input\n",
    "        onnx_model_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"]\n",
    "    )\n",
    "    print(f\"Model exported to ONNX successfully at {onnx_model_path}!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting PyTorch model to ONNX: {e}\")\n",
    "\n",
    "# Step 6: Test ONNX Model\n",
    "try:\n",
    "    session = ort.InferenceSession(onnx_model_path)\n",
    "    onnx_input = pytorch_input.numpy()\n",
    "    onnx_output = session.run(None, {session.get_inputs()[0].name: onnx_input})\n",
    "    onnx_logits = onnx_output[0]\n",
    "    print(\"ONNX Model tested successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing ONNX model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 7: Compare Outputs\n",
    "print(\"\\n--- Comparing PyTorch and ONNX Outputs ---\")\n",
    "try:\n",
    "    # Compare Logits\n",
    "    pytorch_logits_np = pytorch_logits.numpy()\n",
    "    onnx_logits_np = onnx_logits\n",
    "    logit_diff = np.abs(pytorch_logits_np - onnx_logits_np)\n",
    "    print(\"Logit Differences:\", logit_diff)\n",
    "\n",
    "    # Compare Probabilities\n",
    "    pytorch_probabilities = F.softmax(torch.tensor(pytorch_logits_np), dim=1).numpy()\n",
    "    onnx_probabilities = F.softmax(torch.tensor(onnx_logits_np), dim=1).numpy()\n",
    "    probability_diff = np.abs(pytorch_probabilities - onnx_probabilities)\n",
    "    print(\"Probability Differences:\", probability_diff)\n",
    "\n",
    "    # Predicted Classes\n",
    "    pytorch_pred_class = np.argmax(pytorch_probabilities, axis=1)\n",
    "    onnx_pred_class = np.argmax(onnx_probabilities, axis=1)\n",
    "    print(\"PyTorch Predicted Class:\", pytorch_pred_class)\n",
    "    print(\"ONNX Predicted Class:\", onnx_pred_class)\n",
    "    print(\"Do predicted classes match?\", np.array_equal(pytorch_pred_class, onnx_pred_class))\n",
    "\n",
    "    # Save intermediate outputs for further debugging if needed\n",
    "    np.save(\"onnx_logits.npy\", onnx_logits_np)\n",
    "    np.save(\"pytorch_logits.npy\", pytorch_logits_np)\n",
    "except Exception as e:\n",
    "    print(f\"Error comparing outputs: {e}\")\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check accuracy of ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX model...\n",
      "Testing ONNX model with test images...\n",
      "\n",
      "Processing image: 1.jpg\n",
      "Logits: [    -1.0871      0.7644]\n",
      "Probabilities: [     0.1357      0.8643]\n",
      "Predicted Class: Weedy Rice (0.86)\n",
      "\n",
      "Processing image: 2.png\n",
      "Logits: [    -1.5106      1.2516]\n",
      "Probabilities: [     0.0594      0.9406]\n",
      "Predicted Class: Weedy Rice (0.94)\n",
      "\n",
      "Processing image: 3.png\n",
      "Logits: [    -2.5305       1.793]\n",
      "Probabilities: [    0.01308     0.98692]\n",
      "Predicted Class: Weedy Rice (0.99)\n",
      "\n",
      "Processing image: 4.jpg\n",
      "Logits: [     3.2403     -3.1878]\n",
      "Probabilities: [    0.99839    0.001613]\n",
      "Predicted Class: Cultivated Rice (1.00)\n",
      "\n",
      "Processing image: 5.jpg\n",
      "Logits: [     1.2091    -0.79328]\n",
      "Probabilities: [    0.88105     0.11895]\n",
      "Predicted Class: Cultivated Rice (0.88)\n",
      "\n",
      "Processing image: 6.jpg\n",
      "Logits: [     1.2447    -0.92001]\n",
      "Probabilities: [    0.89704     0.10296]\n",
      "Predicted Class: Cultivated Rice (0.90)\n",
      "\n",
      "--- Summary of ONNX Model Predictions ---\n",
      "Image: 1.jpg\n",
      "  Predicted Class: Weedy Rice (Confidence: 0.86)\n",
      "  Logits: [    -1.0871      0.7644]\n",
      "  Probabilities: [     0.1357      0.8643]\n",
      "Image: 2.png\n",
      "  Predicted Class: Weedy Rice (Confidence: 0.94)\n",
      "  Logits: [    -1.5106      1.2516]\n",
      "  Probabilities: [     0.0594      0.9406]\n",
      "Image: 3.png\n",
      "  Predicted Class: Weedy Rice (Confidence: 0.99)\n",
      "  Logits: [    -2.5305       1.793]\n",
      "  Probabilities: [    0.01308     0.98692]\n",
      "Image: 4.jpg\n",
      "  Predicted Class: Cultivated Rice (Confidence: 1.00)\n",
      "  Logits: [     3.2403     -3.1878]\n",
      "  Probabilities: [    0.99839    0.001613]\n",
      "Image: 5.jpg\n",
      "  Predicted Class: Cultivated Rice (Confidence: 0.88)\n",
      "  Logits: [     1.2091    -0.79328]\n",
      "  Probabilities: [    0.88105     0.11895]\n",
      "Image: 6.jpg\n",
      "  Predicted Class: Cultivated Rice (Confidence: 0.90)\n",
      "  Logits: [     1.2447    -0.92001]\n",
      "  Probabilities: [    0.89704     0.10296]\n"
     ]
    }
   ],
   "source": [
    "#after convert to onnx\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define paths\n",
    "onnx_model_dir = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\"  # ONNX model directory\n",
    "onnx_model_path = os.path.join(onnx_model_dir, \"model.onnx\")  # Path to the ONNX model\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    raise FileNotFoundError(f\"ONNX model not found at {onnx_model_path}\")\n",
    "\n",
    "# Define paths\n",
    "test_images_path = r\"C:\\yolov5_env\\WeedyRice_Classification\\test\"  # Test images folder\n",
    "class_names = [\"Cultivated Rice\", \"Weedy Rice\"]  # Class names\n",
    "\n",
    "# Load ONNX model\n",
    "print(\"Loading ONNX model...\")\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, img_size=(224, 224)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize with mean/std\n",
    "    img = np.transpose(img, (2, 0, 1))  # HWC to CHW\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "\n",
    "# Test ONNX model with test images\n",
    "print(\"Testing ONNX model with test images...\")\n",
    "results = []\n",
    "for img_file in os.listdir(test_images_path):\n",
    "    img_path = os.path.join(test_images_path, img_file)\n",
    "    if img_file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        print(f\"\\nProcessing image: {img_file}\")\n",
    "\n",
    "        # Preprocess image\n",
    "        input_tensor = preprocess_image(img_path)\n",
    "\n",
    "        # Run ONNX inference\n",
    "        onnx_output = session.run(None, {session.get_inputs()[0].name: input_tensor})\n",
    "        logits = onnx_output[0]\n",
    "\n",
    "        # Post-process logits to probabilities\n",
    "        probabilities = F.softmax(torch.tensor(logits), dim=1).numpy()[0]\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "        # Save result\n",
    "        results.append({\n",
    "            \"image\": img_file,\n",
    "            \"logits\": logits[0],\n",
    "            \"probabilities\": probabilities,\n",
    "            \"predicted_class\": predicted_class_name,\n",
    "            \"confidence\": probabilities[predicted_class]\n",
    "        })\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Logits: {logits[0]}\")\n",
    "        print(f\"Probabilities: {probabilities}\")\n",
    "        print(f\"Predicted Class: {predicted_class_name} ({probabilities[predicted_class]:.2f})\")\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\n--- Summary of ONNX Model Predictions ---\")\n",
    "for res in results:\n",
    "    print(f\"Image: {res['image']}\")\n",
    "    print(f\"  Predicted Class: {res['predicted_class']} (Confidence: {res['confidence']:.2f})\")\n",
    "    print(f\"  Logits: {res['logits']}\")\n",
    "    print(f\"  Probabilities: {res['probabilities']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert ONNX to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX model...\n",
      "Converting ONNX to TensorFlow SavedModel...\n",
      "INFO:tensorflow:Assets written to: C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow SavedModel saved at C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model!\n",
      "Loading TensorFlow SavedModel...\n",
      "Inspecting model signature...\n",
      "Inputs: ((), {'input': TensorSpec(shape=(1, 3, 224, 224), dtype=tf.float32, name='input')})\n",
      "Outputs: {'output': TensorSpec(shape=(1, 2), dtype=tf.float32, name='output')}\n",
      "Using output key: output\n",
      "Converting to Keras model...\n",
      "Saving as Keras SavedModel...\n",
      "INFO:tensorflow:Assets written to: C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras SavedModel saved at C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model!\n"
     ]
    }
   ],
   "source": [
    "#convert to keras\n",
    "\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "onnx_model_dir = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\"  # ONNX model directory\n",
    "onnx_model_path = os.path.join(onnx_model_dir, \"model.onnx\")  # Path to the ONNX model\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    raise FileNotFoundError(f\"ONNX model not found at {onnx_model_path}\")\n",
    "\n",
    "\n",
    "tf_saved_model_dir = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\"\n",
    "keras_saved_model_dir = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\"\n",
    "\n",
    "# Step 1: Convert ONNX to TensorFlow SavedModel\n",
    "print(\"Loading ONNX model...\")\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "print(\"Converting ONNX to TensorFlow SavedModel...\")\n",
    "os.makedirs(tf_saved_model_dir, exist_ok=True)\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_rep.export_graph(tf_saved_model_dir)\n",
    "print(f\"TensorFlow SavedModel saved at {tf_saved_model_dir}!\")\n",
    "\n",
    "# Step 2: Load TensorFlow SavedModel\n",
    "print(\"Loading TensorFlow SavedModel...\")\n",
    "loaded_model = tf.saved_model.load(tf_saved_model_dir)\n",
    "\n",
    "# Inspect available outputs\n",
    "print(\"Inspecting model signature...\")\n",
    "signatures = loaded_model.signatures[\"serving_default\"]\n",
    "print(\"Inputs:\", signatures.structured_input_signature)\n",
    "print(\"Outputs:\", signatures.structured_outputs)\n",
    "\n",
    "# Extract the correct output key\n",
    "output_key = list(signatures.structured_outputs.keys())[0]  # Use the first available output key\n",
    "print(f\"Using output key: {output_key}\")\n",
    "\n",
    "# Define the wrapped model class\n",
    "class WrappedModel(tf.keras.Model):\n",
    "    def __init__(self, saved_model, output_key):\n",
    "        super(WrappedModel, self).__init__()\n",
    "        self.saved_model = saved_model\n",
    "        self.model_function = saved_model.signatures[\"serving_default\"]\n",
    "        self.output_key = output_key\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure inputs are properly passed\n",
    "        outputs = self.model_function(inputs)\n",
    "        return outputs[self.output_key]\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Define configuration for serialization\n",
    "        return {\"output_key\": self.output_key}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None):\n",
    "        # Recreate the model from the config (requires the saved_model to be passed externally)\n",
    "        raise NotImplementedError(\"Loading this model requires the TensorFlow SavedModel.\")\n",
    "\n",
    "print(\"Converting to Keras model...\")\n",
    "keras_model = WrappedModel(loaded_model, output_key)\n",
    "\n",
    "# Define a concrete input shape\n",
    "sample_input = tf.random.uniform([1, 3, 224, 224])  # Match input size as per ONNX model\n",
    "_ = keras_model(sample_input)  # Build the model by calling it with a sample input\n",
    "\n",
    "# Save as Keras SavedModel\n",
    "print(\"Saving as Keras SavedModel...\")\n",
    "os.makedirs(keras_saved_model_dir, exist_ok=True)\n",
    "tf.keras.models.save_model(keras_model, keras_saved_model_dir, save_format=\"tf\")\n",
    "print(f\"Keras SavedModel saved at {keras_saved_model_dir}!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check accuracy of saved_model.pb (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TensorFlow SavedModel...\n",
      "Inspecting model signature...\n",
      "Inputs: ((), {'input_1': TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name='input_1')})\n",
      "Outputs: {'output_1': TensorSpec(shape=(1, 2), dtype=tf.float32, name='output_1')}\n",
      "Using output key: output_1\n",
      "Testing SavedModel with test images...\n",
      "\n",
      "Processing image: 1.jpg\n",
      "Logits: [    -1.0871      0.7644]\n",
      "Probabilities: [     0.1357      0.8643]\n",
      "Predicted Class: Weedy Rice (0.86)\n",
      "\n",
      "Processing image: 2.png\n",
      "Logits: [    -1.5106      1.2516]\n",
      "Probabilities: [     0.0594      0.9406]\n",
      "Predicted Class: Weedy Rice (0.94)\n",
      "\n",
      "Processing image: 3.png\n",
      "Logits: [    -2.5305       1.793]\n",
      "Probabilities: [    0.01308     0.98692]\n",
      "Predicted Class: Weedy Rice (0.99)\n",
      "\n",
      "Processing image: 4.jpg\n",
      "Logits: [     3.2403     -3.1878]\n",
      "Probabilities: [    0.99839    0.001613]\n",
      "Predicted Class: Cultivated Rice (1.00)\n",
      "\n",
      "Processing image: 5.jpg\n",
      "Logits: [     1.2091    -0.79328]\n",
      "Probabilities: [    0.88105     0.11895]\n",
      "Predicted Class: Cultivated Rice (0.88)\n",
      "\n",
      "Processing image: 6.jpg\n",
      "Logits: [     1.2447    -0.92001]\n",
      "Probabilities: [    0.89704     0.10296]\n",
      "Predicted Class: Cultivated Rice (0.90)\n"
     ]
    }
   ],
   "source": [
    "#check keras model accuracy\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to SavedModel\n",
    "saved_model_dir = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\"\n",
    "test_images_path = r\"C:\\yolov5_env\\WeedyRice_Classification\\test\"  # Test images folder\n",
    "class_names = [\"Cultivated Rice\", \"Weedy Rice\"]  # Class names\n",
    "\n",
    "# Load the SavedModel\n",
    "print(\"Loading TensorFlow SavedModel...\")\n",
    "model = tf.saved_model.load(saved_model_dir)\n",
    "infer = model.signatures[\"serving_default\"]\n",
    "\n",
    "# Inspect available outputs\n",
    "print(\"Inspecting model signature...\")\n",
    "print(\"Inputs:\", infer.structured_input_signature)\n",
    "print(\"Outputs:\", infer.structured_outputs)\n",
    "\n",
    "# Dynamically determine the output key\n",
    "output_key = list(infer.structured_outputs.keys())[0]  # Pick the first available output key\n",
    "print(f\"Using output key: {output_key}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, img_size=(224, 224)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize\n",
    "    img = np.transpose(img, (2, 0, 1))  # Change to channels-first format\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "# Test images\n",
    "print(\"Testing SavedModel with test images...\")\n",
    "for img_file in os.listdir(test_images_path):\n",
    "    img_path = os.path.join(test_images_path, img_file)\n",
    "    if img_file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        print(f\"\\nProcessing image: {img_file}\")\n",
    "        \n",
    "        # Preprocess image\n",
    "        input_tensor = preprocess_image(img_path)\n",
    "\n",
    "        # Run inference\n",
    "        predictions = infer(tf.convert_to_tensor(input_tensor))[output_key].numpy()[0]\n",
    "\n",
    "        # Post-process logits to probabilities\n",
    "        probabilities = tf.nn.softmax(predictions).numpy()\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "        print(f\"Logits: {predictions}\")\n",
    "        print(f\"Probabilities: {probabilities}\")\n",
    "        print(f\"Predicted Class: {predicted_class_name} ({probabilities[predicted_class]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Keras to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model saved to: C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\\model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Path to your SavedModel directory and output TFLite file\n",
    "saved_model_dir = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\saved_model\"\n",
    "output_file = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\\model.tflite\"\n",
    "\n",
    "# Convert the SavedModel to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(output_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite model saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check tflite accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TFLite model...\n",
      "Expected input shape: [  1   3 224 224]\n",
      "Evaluating TFLite model...\n",
      "\n",
      "Processing image: 1.jpg\n",
      "Input tensor shape: (1, 3, 224, 224)\n",
      "Logits: [    -1.0871      0.7644]\n",
      "Probabilities: [     0.1357      0.8643]\n",
      "Predicted Class: Weedy Rice (0.86)\n",
      "Ground Truth: Weedy Rice\n",
      "Correct: True\n",
      "\n",
      "Processing image: 2.png\n",
      "Input tensor shape: (1, 3, 224, 224)\n",
      "Logits: [    -1.5106      1.2516]\n",
      "Probabilities: [     0.0594      0.9406]\n",
      "Predicted Class: Weedy Rice (0.94)\n",
      "Ground Truth: Weedy Rice\n",
      "Correct: True\n",
      "\n",
      "Processing image: 3.png\n",
      "Input tensor shape: (1, 3, 224, 224)\n",
      "Logits: [    -2.5305       1.793]\n",
      "Probabilities: [    0.01308     0.98692]\n",
      "Predicted Class: Weedy Rice (0.99)\n",
      "Ground Truth: Weedy Rice\n",
      "Correct: True\n",
      "\n",
      "Processing image: 4.jpg\n",
      "Input tensor shape: (1, 3, 224, 224)\n",
      "Logits: [     3.2402     -3.1878]\n",
      "Probabilities: [    0.99839    0.001613]\n",
      "Predicted Class: Cultivated Rice (1.00)\n",
      "Ground Truth: Cultivated Rice\n",
      "Correct: True\n",
      "\n",
      "Processing image: 5.jpg\n",
      "Input tensor shape: (1, 3, 224, 224)\n",
      "Logits: [     1.2091    -0.79328]\n",
      "Probabilities: [    0.88105     0.11895]\n",
      "Predicted Class: Cultivated Rice (0.88)\n",
      "Ground Truth: Cultivated Rice\n",
      "Correct: True\n",
      "\n",
      "Processing image: 6.jpg\n",
      "Input tensor shape: (1, 3, 224, 224)\n",
      "Logits: [     1.2447    -0.92001]\n",
      "Probabilities: [    0.89704     0.10296]\n",
      "Predicted Class: Cultivated Rice (0.90)\n",
      "Ground Truth: Cultivated Rice\n",
      "Correct: True\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "Accuracy: 100.00% (6/6)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "tflite_model_path = r\"C:\\yolov5_env\\WeedyRice_Classification\\yolov5\\runs\\train-cls\\exp\\weights\\model.tflite\"\n",
    "test_images_path = r\"C:\\yolov5_env\\WeedyRice_Classification\\test\"  # Folder with test images\n",
    "class_names = [\"Cultivated Rice\", \"Weedy Rice\"]  # Class names\n",
    "ground_truth_labels = {\"1.jpg\": 1, \"2.png\": 1, \"3.png\": 1, \"4.jpg\": 0, \"5.jpg\": 0, \"6.jpg\": 0}  # Example labels\n",
    "\n",
    "# Load the TFLite model\n",
    "print(\"Loading TFLite model...\")\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Verify the input shape\n",
    "expected_input_shape = input_details[0]['shape']  # Shape of input tensor\n",
    "print(f\"Expected input shape: {expected_input_shape}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, img_size=(224, 224)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    img = cv2.resize(img, img_size)  # Resize to model input size\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    img = (img - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize with mean/std\n",
    "    img = np.transpose(img, (2, 0, 1))  # Convert HWC to CHW\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "# Evaluate TFLite model\n",
    "print(\"Evaluating TFLite model...\")\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for img_file in os.listdir(test_images_path):\n",
    "    img_path = os.path.join(test_images_path, img_file)\n",
    "    if img_file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        print(f\"\\nProcessing image: {img_file}\")\n",
    "\n",
    "        # Preprocess the image\n",
    "        input_data = preprocess_image(img_path)\n",
    "\n",
    "        # Check the input shape\n",
    "        print(f\"Input tensor shape: {input_data.shape}\")\n",
    "        if input_data.shape != tuple(expected_input_shape):\n",
    "            raise ValueError(f\"Input shape mismatch. Expected: {expected_input_shape}, Got: {input_data.shape}\")\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "        # Post-process predictions\n",
    "        probabilities = tf.nn.softmax(predictions).numpy()\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "        # Compare with ground truth\n",
    "        ground_truth = ground_truth_labels[img_file]\n",
    "        is_correct = predicted_class == ground_truth\n",
    "        correct_predictions += int(is_correct)\n",
    "        total_predictions += 1\n",
    "\n",
    "        print(f\"Logits: {predictions}\")\n",
    "        print(f\"Probabilities: {probabilities}\")\n",
    "        print(f\"Predicted Class: {predicted_class_name} ({probabilities[predicted_class]:.2f})\")\n",
    "        print(f\"Ground Truth: {class_names[ground_truth]}\")\n",
    "        print(f\"Correct: {is_correct}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct_predictions}/{total_predictions})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
